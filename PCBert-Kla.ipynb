{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f946c29-a1e7-46be-aaac-034783d288b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, AutoTokenizer,BertModel, AutoModel \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da088ce-bed7-4df0-90a2-d47e87bea0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFastaData(fasta_path):\n",
    "\n",
    "    fasta_file = fasta_path\n",
    "\n",
    "\n",
    "    sequences = []\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        sequences.append({\n",
    "            \"id\": record.id,\n",
    "            \"sequence\": str(record.seq)\n",
    "        })\n",
    "\n",
    "    result = []\n",
    "    labels = []\n",
    "\n",
    "    for sequence in sequences:\n",
    "        labels.append(int(sequence[\"id\"]))\n",
    "        result.append(' '.join(sequence[\"sequence\"]))\n",
    "\n",
    "\n",
    "    return result,labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d93d121-fe6b-414d-965f-5e95ef6ef8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_first, trian_labels_first = getFastaData(r'./data/upTrain.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940f7f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "\n",
    "def analyze_protein_sequences(sequence_list):\n",
    "    results = []\n",
    "    \n",
    "    for sequence in sequence_list:\n",
    "\n",
    "        sequence = sequence.replace(\" \", \"\")\n",
    "        \n",
    "        protein_analysis = ProteinAnalysis(sequence)\n",
    "        \n",
    "        molecular_weight = protein_analysis.molecular_weight()\n",
    "        isoelectric_point = protein_analysis.isoelectric_point()\n",
    "        amino_acid_composition = list(protein_analysis.get_amino_acids_percent().values())\n",
    "        secondary_structure_fraction = list(protein_analysis.secondary_structure_fraction())\n",
    "        hydrophobicity = protein_analysis.gravy()\n",
    "        charge_at_pH_7 = protein_analysis.charge_at_pH(7.0)\n",
    "        \n",
    "        features = [\n",
    "            molecular_weight,\n",
    "            isoelectric_point,\n",
    "            *amino_acid_composition,  \n",
    "            *secondary_structure_fraction,  \n",
    "            hydrophobicity,\n",
    "            charge_at_pH_7\n",
    "        ]\n",
    "        \n",
    "        results.append(features)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d0031d-0598-4879-955c-85997160c93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_weights = F.softmax(self.W(x), dim=1)\n",
    "        output = attn_weights * x\n",
    "        return output\n",
    "\n",
    "\n",
    "class myModel(torch.nn.Module):\n",
    "    def __init__(self,esm2):\n",
    "        super(myModel,self).__init__()\n",
    "        self.esm2 = esm2\n",
    "        self.fc1 = torch.nn.Linear(1051,32)\n",
    "        self.fc2 = torch.nn.Linear(32,8)\n",
    "        self.fc3 = torch.nn.Linear(8,1)\n",
    "        \n",
    "        self.att1 = Attention(32)\n",
    "   \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU() \n",
    "        self.bn1 = torch.nn.BatchNorm1d(64)  \n",
    "        self.dropout1 = nn.Dropout(p=0.1)\n",
    "        self.dropout2 = nn.Dropout(p=0.3)\n",
    "    def forward(self,x,fe):\n",
    "        outputs_ = self.esm2(**inputs)\n",
    "\n",
    "        x = outputs_.last_hidden_state[:, 0, :]\n",
    "        x = torch.cat((x, fe), dim=1)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.att1(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc444e46-7f46-4636-8d7d-5cfb4ee41ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, matthews_corrcoef, recall_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def comp_result(y_test, y_pred, y_proba):\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "    print(f\"MCC: {mcc:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    \n",
    "    return accuracy,f1,auc,mcc,recall,precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d93279-2083-464f-bf30-fd634841e9b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import copy as cp\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "kf_in = 0\n",
    "feature = np.array(train_features_first)\n",
    "label = np.array(trian_labels_first)\n",
    "loaded_scaler = joblib.load('./scaler_model.pkl')\n",
    "\n",
    "for train_index, test_index in kf.split(feature):\n",
    "    \n",
    "    save_model = None\n",
    "    best_accuracy = 0\n",
    "    best_f1 = 0\n",
    "    best_auc = 0\n",
    "    best_mcc = 0\n",
    "    best_recall = 0\n",
    "    best_precision = 0\n",
    "    kf_in += 1\n",
    "\n",
    "    print('kf',kf_in)\n",
    "    train_features, test_features = list(feature[train_index]), list(feature[test_index])\n",
    "    train_labels, test_labels = list(label[train_index]), list(label[test_index])\n",
    "\n",
    "    device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    cache_directory = r\"/home/hqzhang/neuropeptide prediction/model\"\n",
    "\n",
    "    tokenizer_ = AutoTokenizer.from_pretrained(\"Rostlab/prot_bert\", cache_dir=cache_directory)\n",
    "    esm2 = AutoModel.from_pretrained(\"Rostlab/prot_bert\", cache_dir=cache_directory)\n",
    "    del esm2.encoder.layer[4:]\n",
    "    model = myModel(esm2)\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "\n",
    "    batch_size = 4  \n",
    "    data_loader = DataLoader(list(zip(train_features, train_labels)), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_data_loader = DataLoader(list(zip(test_features, test_labels)), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    num_epochs = 30\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_input, batch_labels in data_loader:\n",
    "            \n",
    "            nol_feature = analyze_protein_sequences(batch_input)\n",
    "            normalized_nol_feature = loaded_scaler.transform(nol_feature)\n",
    "            normalized_nol_feature = torch.tensor(normalized_nol_feature).float()\n",
    "            normalized_nol_feature = normalized_nol_feature.to(device)\n",
    "            inputs = tokenizer_(batch_input, return_tensors='pt', padding=True, truncation=True)\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            outputs = model(inputs,normalized_nol_feature)\n",
    "            outputs = outputs.view(-1)\n",
    "            loss = criterion(outputs.to('cpu'), batch_labels.to(torch.float32))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        model.eval()  \n",
    "        test_loss = 0\n",
    "        correct_predictions = 0\n",
    "        \n",
    "        \n",
    "        test_labels_com = []\n",
    "        predict_labels_com = []\n",
    "        test_outputs_com = []\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for test_batch_input, test_batch_labels in test_data_loader:\n",
    "                \n",
    "                nol_feature = analyze_protein_sequences(test_batch_input)\n",
    "                normalized_nol_feature = loaded_scaler.transform(nol_feature)\n",
    "                normalized_nol_feature = torch.tensor(normalized_nol_feature).float()\n",
    "                normalized_nol_feature = normalized_nol_feature.to(device)\n",
    "                \n",
    "                inputs = tokenizer_(test_batch_input, return_tensors='pt', padding=True, truncation=True)\n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "                test_outputs = model(inputs,normalized_nol_feature)\n",
    "                test_outputs = test_outputs.view(-1)\n",
    "                test_loss += criterion(test_outputs.to('cpu'), test_batch_labels.to(torch.float32)).item()\n",
    "                predictions = (test_outputs > 0.5).float()  \n",
    "                correct_predictions += (predictions.to('cpu') == test_batch_labels).sum().item()\n",
    "                \n",
    "                predict_labels_com.extend(predictions.tolist())\n",
    "                test_labels_com.extend(test_batch_labels.tolist())\n",
    "                test_outputs_com.extend(test_outputs.tolist())\n",
    "        \n",
    "        average_test_loss = test_loss / len(test_data_loader)\n",
    "        accuracy = correct_predictions / len(test_features)\n",
    "        \n",
    "        predict_labels_com = np.array(predict_labels_com)\n",
    "        test_labels_com = np.array(test_labels_com)\n",
    "        test_outputs_com = np.array(test_outputs_com)\n",
    "        temp_accuracy_test,temp_f1,temp_auc,temp_mcc,temp_recall,temp_precision = comp_result(test_labels_com,predict_labels_com,test_outputs_com)\n",
    "        if temp_accuracy_test > best_accuracy:\n",
    "            best_accuracy = temp_accuracy_test\n",
    "            best_f1 = temp_f1\n",
    "            best_auc = temp_auc\n",
    "            best_mcc = temp_mcc\n",
    "            best_recall = temp_recall\n",
    "            best_precision = temp_precision\n",
    "            save_model = cp.deepcopy(model)\n",
    "        average_train_loss = total_loss / len(data_loader)\n",
    "        print('Epoch [{}/{}], Train Loss: {:.4f}, Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
    "            epoch + 1, num_epochs, average_train_loss, average_test_loss, accuracy * 100))\n",
    "        model.train()\n",
    "    torch.save(save_model, r'./result/model_base_BBK_%d.pth' % kf_in)\n",
    "    print('=====================================')\n",
    "    print('kf',kf_in,\"best result:\")\n",
    "    print(best_accuracy)\n",
    "    print(best_f1)\n",
    "    print(best_auc)\n",
    "    print(best_mcc)\n",
    "    print(best_recall)\n",
    "    print(best_precision)\n",
    "    print('=====================================')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
